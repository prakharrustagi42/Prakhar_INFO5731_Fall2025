{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakharrustagi42/Prakhar_INFO5731_Fall2025/blob/main/Rustagi_Prakhar_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13324f89-67cb-4c38-d955-fa1e0d2b28a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Collecting typing_extensions~=4.14.0 (from selenium)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing_extensions, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.35.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.14.1 wsproto-1.2.0\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (2.32.4)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->webdriver-manager) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->webdriver-manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->webdriver-manager) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->webdriver-manager) (2025.8.3)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Installing collected packages: webdriver-manager\n",
            "Successfully installed webdriver-manager-4.0.2\n",
            "Starting Firefox WebDriver...\n",
            "Opening page: https://www.imdb.com/title/tt5950044/reviews/\n",
            "Looking for 'See all' button...\n",
            "Clicked 'See all' button\n",
            "Scrolling to load reviews...\n",
            "   Scroll 1 complete\n",
            "   Scroll 2 complete\n",
            "   Scroll 3 complete\n",
            "   Scroll 4 complete\n",
            "   Scroll 5 complete\n",
            "   Scroll 6 complete\n",
            "   Scroll 7 complete\n",
            "   Scroll 8 complete\n",
            "   Scroll 9 complete\n",
            "   Scroll 10 complete\n",
            "   Scroll 11 complete\n",
            "   Scroll 12 complete\n",
            "   Scroll 13 complete\n",
            "   Scroll 14 complete\n",
            "   Scroll 15 complete\n",
            "   Scroll 16 complete\n",
            "   Scroll 17 complete\n",
            "   Scroll 18 complete\n",
            "   Scroll 19 complete\n",
            "   Scroll 20 complete\n",
            "   Scroll 21 complete\n",
            "   Scroll 22 complete\n",
            "   Scroll 23 complete\n",
            "   Scroll 24 complete\n",
            "   Scroll 25 complete\n",
            "   Scroll 26 complete\n",
            "   Scroll 27 complete\n",
            "   Scroll 28 complete\n",
            "   Scroll 29 complete\n",
            "   Scroll 30 complete\n",
            "Expanding spoiler reviews (if any)...\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium\n",
        "!pip install webdriver-manager\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.firefox.service import Service\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.firefox import GeckoDriverManager\n",
        "import time\n",
        "\n",
        "\n",
        "def scrape_imdb_reviews(url, scroll_pause=2, max_scrolls=20):\n",
        "    \"\"\"Scrape IMDb reviews after clicking 'See all' and scrolling until all load.\"\"\"\n",
        "\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    options.add_argument(\"--headless\")\n",
        "\n",
        "    print(\"Starting Firefox WebDriver...\")\n",
        "    driver = webdriver.Firefox(\n",
        "        options=options\n",
        "    )\n",
        "    wait = WebDriverWait(driver, 10)\n",
        "\n",
        "    print(f\"Opening page: {url}\")\n",
        "    driver.get(url)\n",
        "\n",
        "    # 1. Click the main \"See all\" button\n",
        "    try:\n",
        "        print(\"Looking for 'See all' button...\")\n",
        "        see_all_btn = wait.until(\n",
        "            EC.element_to_be_clickable((By.XPATH, \"//span[contains(@class,'ipc-see-more__text') and text()='See all']\"))\n",
        "        )\n",
        "        driver.execute_script(\"arguments[0].click();\", see_all_btn)\n",
        "        print(\"Clicked 'See all' button\")\n",
        "        time.sleep(2)\n",
        "    except Exception:\n",
        "        print(\"Could not find 'See all' button, scraping current page only\")\n",
        "\n",
        "    # 2. Scroll to load all reviews\n",
        "    print(\"Scrolling to load reviews...\")\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    scrolls = 0\n",
        "    while scrolls < max_scrolls:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(scroll_pause)\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            print(\"Reached bottom of page\")\n",
        "            break\n",
        "        last_height = new_height\n",
        "        scrolls += 1\n",
        "        print(f\"   Scroll {scrolls} complete\")\n",
        "\n",
        "    # 3. Expand spoiler reviews\n",
        "    print(\"Expanding spoiler reviews (if any)...\")\n",
        "    spoiler_buttons = driver.find_elements(By.CSS_SELECTOR, \"button.review-spoiler-button\")\n",
        "    expanded = 0\n",
        "    for btn in spoiler_buttons:\n",
        "        try:\n",
        "            driver.execute_script(\"arguments[0].click();\", btn)\n",
        "            expanded += 1\n",
        "            time.sleep(0.3)\n",
        "        except Exception:\n",
        "            pass\n",
        "    print(f\"Expanded {expanded} spoiler reviews\")\n",
        "\n",
        "    # 4. Collect all review containers\n",
        "    print(\"Extracting review data...\")\n",
        "    reviews = []\n",
        "    review_blocks = driver.find_elements(By.CSS_SELECTOR, \"article.user-review-item\")\n",
        "\n",
        "    for idx, block in enumerate(review_blocks, 1):\n",
        "        try:\n",
        "            rating = block.find_element(By.CSS_SELECTOR, \".ipc-rating-star--rating\").text\n",
        "        except:\n",
        "            rating = None\n",
        "        try:\n",
        "            title = block.find_element(By.CSS_SELECTOR, \"div[data-testid='review-summary']\").text\n",
        "        except:\n",
        "            title = None\n",
        "        try:\n",
        "            author = block.find_element(By.CSS_SELECTOR, \"a[data-testid='author-link']\").text\n",
        "        except:\n",
        "            author = None\n",
        "        try:\n",
        "            date = block.find_element(By.CSS_SELECTOR, \"li.review-date\").text\n",
        "        except:\n",
        "            date = None\n",
        "        try:\n",
        "            text = block.find_element(By.CSS_SELECTOR, \"div.ipc-html-content\").text\n",
        "        except:\n",
        "            text = None\n",
        "        try:\n",
        "            helpful = block.find_element(By.CSS_SELECTOR, \".ipc-voting__label__count--up\").text\n",
        "        except:\n",
        "            helpful = None\n",
        "\n",
        "        reviews.append({\n",
        "            \"rating\": rating,\n",
        "            \"title\": title,\n",
        "            \"author\": author,\n",
        "            \"date\": date,\n",
        "            \"text\": text,\n",
        "            \"helpful\": helpful\n",
        "        })\n",
        "\n",
        "        if idx % 50 == 0:\n",
        "            print(f\"   Extracted {len(reviews)} reviews...\")\n",
        "        if idx % 1000 == 0:\n",
        "            break\n",
        "\n",
        "    driver.quit()\n",
        "    print(f\"Finished! Extracted {len(reviews)} reviews total.\")\n",
        "    return reviews\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    imdb_url = \"https://www.imdb.com/title/tt5950044/reviews/\"\n",
        "    data = scrape_imdb_reviews(imdb_url, scroll_pause=2, max_scrolls=30)\n",
        "    data = pd.DataFrame(data)\n",
        "\n",
        "    data.to_csv(\"imdb_reviews.csv\", index=False)\n",
        "    # for idx, r in enumerate(data[:5], 1):  # show first 5 reviews\n",
        "    #     print(f\"\\n{idx}. {r['author']} ({r['date']}) - {r['rating']}/10\")\n",
        "    #     print(f\"   {r['title']}\")\n",
        "    #     print(f\"   {r['text'][:120]}...\")\n",
        "    #     print(f\"   Helpful: {r['helpful']}\")\n",
        "\n",
        "    print(f\"\\nTotal reviews scraped: {len(data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QX6bJjGWXY9"
      },
      "outputs": [],
      "source": [
        "# Question 2: Complete cleaning pipeline (paste into your .ipynb)\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "# Try to import nltk components; fall back gracefully if not available.\n",
        "_nltk_available = True\n",
        "try:\n",
        "    import nltk\n",
        "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "    from nltk.corpus import stopwords, wordnet\n",
        "    from nltk import pos_tag\n",
        "except Exception:\n",
        "    _nltk_available = False\n",
        "    nltk = None\n",
        "\n",
        "# If NLTK is present, attempt to ensure required corpora are available.\n",
        "if _nltk_available:\n",
        "    resources = {\n",
        "        \"punkt\": \"tokenizers/punkt\",\n",
        "        \"stopwords\": \"corpora/stopwords\",\n",
        "        \"wordnet\": \"corpora/wordnet\",\n",
        "        \"averaged_perceptron_tagger\": \"taggers/averaged_perceptron_tagger\",\n",
        "        \"omw-1.4\": \"corpora/omw-1.4\",\n",
        "    }\n",
        "    for name, path in resources.items():\n",
        "        try:\n",
        "            nltk.data.find(path)\n",
        "        except LookupError:\n",
        "            try:\n",
        "                print(f\"Downloading NLTK resource: {name} ...\")\n",
        "                nltk.download(name)\n",
        "            except Exception:\n",
        "                print(f\"Could not download NLTK resource: {name}. Falling back where necessary.\")\n",
        "\n",
        "# Fallback stopwords (close to NLTK's list) if stopwords resource not available\n",
        "FALLBACK_STOPWORDS = {\n",
        " 'i','me','my','myself','we','our','ours','ourselves','you',\"you're\",\"you've\",\"you'll\",\"you'd\",\n",
        " 'your','yours','yourself','yourselves','he','him','his','himself','she',\"she's\",'her','hers','herself',\n",
        " 'it',\"it's\",'its','itself','they','them','their','theirs','themselves','what','which','who','whom',\n",
        " 'this','that',\"that'll\",'these','those','am','is','are','was','were','be','been','being','have','has','had',\n",
        " 'having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while',\n",
        " 'of','at','by','for','with','about','against','between','into','through','during','before','after',\n",
        " 'above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once',\n",
        " 'here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such',\n",
        " 'no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just','don',\"don't\",\n",
        " 'should',\"should've\",'now','d','ll','m','o','re','ve','y','ain','aren',\"aren't\",'couldn',\"couldn't\",'didn',\n",
        " \"didn't\",'doesn',\"doesn't\",'hadn',\"hadn't\",'hasn',\"hasn't\",'haven',\"haven't\",'isn',\"isn't\",'ma','mightn',\n",
        " \"mightn't\",'mustn',\"mustn't\",'needn',\"needn't\",'shan',\"shan't\",'shouldn',\"shouldn't\",'wasn',\"wasn't\",\n",
        " 'weren',\"weren't\",'won',\"won't\",'wouldn',\"wouldn't\"\n",
        "}\n",
        "\n",
        "# Helpers\n",
        "def find_text_column(df: pd.DataFrame) -> str:\n",
        "    candidates = ['text', 'review', 'reviewText', 'content', 'review_text', 'review_body']\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == object:\n",
        "            return col\n",
        "    raise ValueError(\"No suitable text column found. Ensure your CSV has a text column.\")\n",
        "\n",
        "def remove_html_tags(text: str) -> str:\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    return re.sub(r'<[^>]+>', ' ', text)\n",
        "\n",
        "def remove_special_characters(text: str) -> str:\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', ' ', text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def remove_numbers(text: str) -> str:\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    return re.sub(r'\\d+', ' ', text).strip()\n",
        "\n",
        "def lowercase_text(text: str) -> str:\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    return text.lower()\n",
        "\n",
        "def tokenize_text(text: str) -> List[str]:\n",
        "    if _nltk_available:\n",
        "        try:\n",
        "            return [tok for tok in nltk.word_tokenize(text)]\n",
        "        except Exception:\n",
        "            pass\n",
        "    return re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "def remove_stopwords_from_tokens(tokens: List[str], stop_words_set: set) -> List[str]:\n",
        "    return [t for t in tokens if t.lower() not in stop_words_set and len(t) > 1]\n",
        "\n",
        "def get_wordnet_pos(treebank_tag: str):\n",
        "    if _nltk_available:\n",
        "        if treebank_tag.startswith('J'): return wordnet.ADJ\n",
        "        if treebank_tag.startswith('V'): return wordnet.VERB\n",
        "        if treebank_tag.startswith('N'): return wordnet.NOUN\n",
        "        if treebank_tag.startswith('R'): return wordnet.ADV\n",
        "        return wordnet.NOUN\n",
        "    else:\n",
        "        # fallback: return 'n' (noun) string if wordnet not present\n",
        "        return 'n'\n",
        "\n",
        "def naive_lemmatize(word: str) -> str:\n",
        "    if len(word) <= 3: return word\n",
        "    if word.endswith('ies'): return word[:-3] + 'y'\n",
        "    if word.endswith('ing') and len(word) > 4: return word[:-3]\n",
        "    if word.endswith('ed') and len(word) > 3: return word[:-2]\n",
        "    if word.endswith('s') and len(word) > 3: return word[:-1]\n",
        "    return word\n",
        "\n",
        "def stem_tokens(tokens: List[str]) -> List[str]:\n",
        "    if _nltk_available:\n",
        "        try:\n",
        "            ps = PorterStemmer()\n",
        "            return [ps.stem(t) for t in tokens]\n",
        "        except Exception:\n",
        "            pass\n",
        "    out = []\n",
        "    for w in tokens:\n",
        "        if len(w) > 4 and w.endswith('ing'):\n",
        "            out.append(w[:-3])\n",
        "        elif len(w) > 3 and w.endswith('ed'):\n",
        "            out.append(w[:-2])\n",
        "        elif len(w) > 3 and w.endswith('s'):\n",
        "            out.append(w[:-1])\n",
        "        else:\n",
        "            out.append(w)\n",
        "    return out\n",
        "\n",
        "def lemmatize_tokens(tokens: List[str]) -> List[str]:\n",
        "    if _nltk_available:\n",
        "        try:\n",
        "            wnl = WordNetLemmatizer()\n",
        "            try:\n",
        "                pos_tags = pos_tag(tokens)\n",
        "            except Exception:\n",
        "                pos_tags = [(t, 'n') for t in tokens]\n",
        "            lemm = []\n",
        "            for token, tag in pos_tags:\n",
        "                wn_pos = get_wordnet_pos(tag)\n",
        "                try:\n",
        "                    lemm.append(wnl.lemmatize(token, wn_pos))\n",
        "                except Exception:\n",
        "                    lemm.append(wnl.lemmatize(token))\n",
        "            return lemm\n",
        "        except Exception:\n",
        "            pass\n",
        "    return [naive_lemmatize(t) for t in tokens]\n",
        "\n",
        "# Load CSV or fallback sample (for demonstration)\n",
        "csv_path = \"imdb_reviews.csv\"\n",
        "if os.path.exists(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"Loaded '{csv_path}' with {len(df)} rows.\")\n",
        "else:\n",
        "    print(f\"'{csv_path}' not found; using sample data for demonstration.\")\n",
        "    sample_texts = [\n",
        "        \"I loved this movie!!! It was AMAZING. 10/10 <br> Highly recommended.\",\n",
        "        \"Terrible. Waste of time. The acting was awful; plot had many holes. 2/10.\",\n",
        "        \"It was OK — not great, not awful. Some scenes were funny, others boring.\",\n",
        "        \"Plot twist was unexpected. Acting by John Doe was superb. Can't wait for part 2!\",\n",
        "        \"Special effects 100% amazing; music (score) was awesome. But the ending? Meh...\"\n",
        "    ]\n",
        "    df = pd.DataFrame({\"text\": sample_texts})\n",
        "\n",
        "text_col = find_text_column(df)\n",
        "print(\"Using text column:\", text_col)\n",
        "\n",
        "df[text_col] = df[text_col].fillna('').astype(str)\n",
        "\n",
        "if _nltk_available:\n",
        "    try:\n",
        "        STOPWORDS = set(stopwords.words('english'))\n",
        "        print(\"Using NLTK stopwords.\")\n",
        "    except Exception:\n",
        "        STOPWORDS = FALLBACK_STOPWORDS\n",
        "        print(\"NLTK stopwords unavailable; using fallback list.\")\n",
        "else:\n",
        "    STOPWORDS = FALLBACK_STOPWORDS\n",
        "    print(\"NLTK not available; using fallback stopwords list.\")\n",
        "\n",
        "# Apply cleaning steps\n",
        "df['text_no_html'] = df[text_col].apply(remove_html_tags)\n",
        "df['text_no_special'] = df['text_no_html'].apply(remove_special_characters)       # step 1 (noise/punct)\n",
        "df['text_no_numbers'] = df['text_no_special'].apply(remove_numbers)               # step 2 (numbers)\n",
        "df['text_lower'] = df['text_no_numbers'].apply(lowercase_text)                   # step 4 (lowercase)\n",
        "df['tokens'] = df['text_lower'].apply(tokenize_text)\n",
        "df['tokens_nostop'] = df['tokens'].apply(lambda toks: remove_stopwords_from_tokens(toks, STOPWORDS))  # step 3 (stopwords)\n",
        "df['text_nostop'] = df['tokens_nostop'].apply(lambda toks: \" \".join(toks))\n",
        "\n",
        "# Stemming (step 5)\n",
        "df['stemmed_tokens'] = df['tokens_nostop'].apply(stem_tokens)\n",
        "df['text_stemmed'] = df['stemmed_tokens'].apply(lambda toks: \" \".join(toks))\n",
        "\n",
        "# Lemmatization (step 6)\n",
        "df['lemmatized_tokens'] = df['tokens_nostop'].apply(lemmatize_tokens)\n",
        "df['text_lemmatized'] = df['lemmatized_tokens'].apply(lambda toks: \" \".join(toks))\n",
        "\n",
        "# Final cleaned column and save\n",
        "df['clean_text'] = df['text_lemmatized']\n",
        "out_csv = \"imdb_reviews_cleaned.csv\"\n",
        "df.to_csv(out_csv, index=False)\n",
        "print(f\"Saved cleaned file to: {out_csv}\")\n",
        "\n",
        "# Show sample rows (first 5) demonstrating each major step\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "cols_to_show = [text_col, 'text_no_html', 'text_no_special', 'text_no_numbers',\n",
        "                'text_lower', 'text_nostop', 'text_stemmed', 'text_lemmatized', 'clean_text']\n",
        "print(df[cols_to_show].head(5).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0oOSlsOS0cq"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "# Question 3: Syntax and Structure Analysis\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the cleaned CSV from Question 2\n",
        "df = pd.read_csv(\"imdb_reviews_cleaned.csv\")\n",
        "\n",
        "# Ensure text column exists\n",
        "if \"clean_text\" not in df.columns:\n",
        "    raise ValueError(\"clean_text column not found. Run Question 2 cleaning first!\")\n",
        "\n",
        "# Work with first few rows for demonstration (to avoid huge output)\n",
        "texts = df[\"clean_text\"].dropna().astype(str).tolist()[:5]\n",
        "\n",
        "# -------------------------------\n",
        "# (1) POS Tagging & Counting\n",
        "# -------------------------------\n",
        "pos_counts = Counter()\n",
        "\n",
        "for doc in nlp.pipe(texts):\n",
        "    for token in doc:\n",
        "        if token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]:\n",
        "            pos_counts[token.pos_] += 1\n",
        "\n",
        "print(\"=== POS Tag Counts ===\")\n",
        "for tag in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]:\n",
        "    print(f\"{tag}: {pos_counts[tag]}\")\n",
        "\n",
        "# -------------------------------\n",
        "# (2) Constituency & Dependency Parsing\n",
        "# -------------------------------\n",
        "# spaCy does dependency parsing natively, but not constituency trees.\n",
        "# For constituency parsing, we use benepar (Berkeley Neural Parser).\n",
        "# Install once: !pip install benepar\n",
        "try:\n",
        "    import benepar\n",
        "    if \"benepar_en3\" not in spacy.util.get_installed_models():\n",
        "        import subprocess\n",
        "        subprocess.run([\"python\", \"-m\", \"benepar.download\", \"benepar_en3\"])\n",
        "    nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "    benepar_available = True\n",
        "except Exception as e:\n",
        "    print(\"Benepar not available, skipping constituency parsing.\")\n",
        "    benepar_available = False\n",
        "\n",
        "# Example sentence for explanation\n",
        "example_sentence = texts[0]\n",
        "doc = nlp(example_sentence)\n",
        "\n",
        "print(\"\\n=== Example Sentence ===\")\n",
        "print(example_sentence)\n",
        "\n",
        "print(\"\\n--- Dependency Parsing Tree ---\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<12} {token.dep_:<10} {token.head.text:<12} POS={token.pos_}\")\n",
        "\n",
        "if benepar_available:\n",
        "    print(\"\\n--- Constituency Parsing Tree ---\")\n",
        "    sent = list(doc.sents)[0]\n",
        "    print(sent._.parse_string)\n",
        "\n",
        "# Explanation (write-up for report)\n",
        "print(\"\"\"\n",
        "Explanation:\n",
        "- Dependency Parsing shows how words depend on each other (head–dependent relations).\n",
        "  Example: in 'loved movie', 'loved' is the root verb and 'movie' is its object.\n",
        "- Constituency Parsing shows how words group into larger units (phrases).\n",
        "  Example: (S (NP I) (VP loved (NP this movie))) means the sentence has a subject NP (I)\n",
        "  and a VP (loved this movie).\n",
        "\"\"\")\n",
        "\n",
        "# -------------------------------\n",
        "# (3) Named Entity Recognition (NER)\n",
        "# -------------------------------\n",
        "entity_counter = Counter()\n",
        "all_entities = []\n",
        "\n",
        "for doc in nlp.pipe(texts):\n",
        "    for ent in doc.ents:\n",
        "        entity_counter[ent.label_] += 1\n",
        "        all_entities.append((ent.text, ent.label_))\n",
        "\n",
        "print(\"\\n=== Named Entities and Counts ===\")\n",
        "for label, count in entity_counter.items():\n",
        "    print(f\"{label}: {count}\")\n",
        "\n",
        "# Save entities to a CSV for clarity\n",
        "entities_df = pd.DataFrame(all_entities, columns=[\"Entity\", \"Label\"])\n",
        "entities_df.to_csv(\"imdb_named_entities.csv\", index=False)\n",
        "print(\"\\nNamed entities saved to imdb_named_entities.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ],
      "metadata": {
        "id": "EcVqy1yj3wja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4 (20 points)."
      ],
      "metadata": {
        "id": "kEdcyHX8VaDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ],
      "metadata": {
        "id": "1Ung5_YW3C6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ],
      "metadata": {
        "id": "CTOfUpatronW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4 PART-1: Scrape GitHub Marketplace (actions) - requests + BeautifulSoup\n",
        "# NOTE: Run in a notebook. Install requirements if needed: !pip install requests beautifulsoup4 pandas\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "import re\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "\n",
        "BASE_URL = \"https://github.com/marketplace\"\n",
        "SEARCH_PARAMS = {\"type\": \"actions\"}  # marketplace filters\n",
        "OUTPUT_CSV = \"github_marketplace_actions.csv\"\n",
        "TARGET_COUNT = 1000        # number of products to collect\n",
        "MAX_PAGES = 200            # safe upper limit\n",
        "DELAY_RANGE = (1.0, 2.5)   # seconds between requests (randomized)\n",
        "REQUEST_TIMEOUT = 15       # seconds\n",
        "\n",
        "# create a requests session with retries\n",
        "session = requests.Session()\n",
        "retries = Retry(total=5, backoff_factor=1, status_forcelist=(429, 500, 502, 503, 504))\n",
        "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\n",
        "    \"User-Agent\": \"Mozilla/5.0 (compatible; StudentScraper/1.0; +https://your-university.edu)\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "})\n",
        "\n",
        "def is_valid_url(url: str) -> bool:\n",
        "    return isinstance(url, str) and url.startswith(\"https://github.com/\")\n",
        "\n",
        "def parse_marketplace_page(html, page_number):\n",
        "    \"\"\"\n",
        "    Parse a marketplace page and return list of product dicts:\n",
        "    {name, description, url, page_number}\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    items = []\n",
        "    # Marketplace cards have changed over time; try a few selectors robustly\n",
        "    # Common patterns: articles with role=\"article\" or divs with 'marketplace-card' classes\n",
        "    card_selectors = [\n",
        "        \"article\",\n",
        "        \"div.marketplace-card\",\n",
        "        \"div.d-flex.col-12.col-md-6\"  # fallback\n",
        "    ]\n",
        "    cards = []\n",
        "    for sel in card_selectors:\n",
        "        found = soup.select(sel)\n",
        "        if found:\n",
        "            cards = found\n",
        "            break\n",
        "\n",
        "    # If no cards found, try to locate links under .col- or .Box-row\n",
        "    if not cards:\n",
        "        cards = soup.select(\"div.Box-row a[href*='/marketplace/']\") or soup.select(\"a[href*='/marketplace/']\")\n",
        "\n",
        "    for card in cards:\n",
        "        try:\n",
        "            # Find name\n",
        "            name_tag = card.select_one(\"h3\") or card.select_one(\"a[href*='/marketplace/']\")\n",
        "            name = name_tag.get_text(strip=True) if name_tag else None\n",
        "\n",
        "            # Find href (product page) - search for first github.com link in card\n",
        "            link_tag = card.find(\"a\", href=True)\n",
        "            url = None\n",
        "            if link_tag:\n",
        "                href = link_tag[\"href\"]\n",
        "                # some hrefs are relative like /marketplace/actions/some-action\n",
        "                url = urljoin(\"https://github.com\", href)\n",
        "\n",
        "            # Description\n",
        "            desc_tag = card.select_one(\"p\") or card.select_one(\"div[data-view-component='true'] p\")\n",
        "            description = desc_tag.get_text(\" \", strip=True) if desc_tag else \"\"\n",
        "\n",
        "            # Basic heuristics: skip non-marketplace links\n",
        "            if url and \"/marketplace\" in url and name:\n",
        "                items.append({\n",
        "                    \"name\": name,\n",
        "                    \"description\": description,\n",
        "                    \"url\": url,\n",
        "                    \"page_number\": page_number\n",
        "                })\n",
        "        except Exception:\n",
        "            # skip any card we can't parse\n",
        "            continue\n",
        "    return items\n",
        "\n",
        "def fetch_marketplace_pages(target_count=TARGET_COUNT):\n",
        "    collected = []\n",
        "    page = 1\n",
        "    while len(collected) < target_count and page <= MAX_PAGES:\n",
        "        # Construct page URL with pagination param: marketplace uses `?type=actions&query=...&page=2`\n",
        "        params = SEARCH_PARAMS.copy()\n",
        "        params[\"page\"] = page\n",
        "        print(f\"[INFO] Fetching page {page} (collected {len(collected)}/{target_count}) ...\")\n",
        "        try:\n",
        "            r = session.get(BASE_URL, params=params, timeout=REQUEST_TIMEOUT)\n",
        "            if r.status_code != 200:\n",
        "                print(f\"[WARN] Received status {r.status_code} for page {page}. Sleeping and retrying...\")\n",
        "                time.sleep(random.uniform(*DELAY_RANGE))\n",
        "                page += 1\n",
        "                continue\n",
        "\n",
        "            page_items = parse_marketplace_page(r.text, page)\n",
        "            print(f\"   Parsed {len(page_items)} items from page {page}\")\n",
        "            # Add new items that aren't already present (check by URL)\n",
        "            existing_urls = set(x[\"url\"] for x in collected if x.get(\"url\"))\n",
        "            for it in page_items:\n",
        "                if it.get(\"url\") and it[\"url\"] not in existing_urls:\n",
        "                    collected.append(it)\n",
        "                    if len(collected) >= target_count:\n",
        "                        break\n",
        "\n",
        "            # polite delay\n",
        "            time.sleep(random.uniform(*DELAY_RANGE))\n",
        "            page += 1\n",
        "\n",
        "            # if no items parsed for several pages, we may be blocked or markup changed -> break\n",
        "            if page_items == [] and page > 5:\n",
        "                print(\"[WARN] No items parsed from this or recent pages. The site may be JS-rendered or markup changed.\")\n",
        "                break\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"[ERROR] Request failed for page {page}: {e}. Backing off...\")\n",
        "            time.sleep(5 + random.random()*5)\n",
        "            page += 1\n",
        "            continue\n",
        "\n",
        "    return collected\n",
        "\n",
        "# Run the fetch\n",
        "collected = fetch_marketplace_pages(target_count=TARGET_COUNT)\n",
        "print(f\"[DONE] Collected {len(collected)} items.\")\n",
        "\n",
        "# Save to CSV (columns: product name, description, url, page number)\n",
        "df = pd.DataFrame(collected)\n",
        "df = df[[\"name\", \"description\", \"url\", \"page_number\"]]\n",
        "df.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"Saved results to {OUTPUT_CSV}\")\n"
      ],
      "metadata": {
        "id": "4dtco9K--ks6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ],
      "metadata": {
        "id": "3WeD70ty3Gui"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYRO5Cn8bYwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ],
      "metadata": {
        "id": "JbTa-jDS-KFI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}